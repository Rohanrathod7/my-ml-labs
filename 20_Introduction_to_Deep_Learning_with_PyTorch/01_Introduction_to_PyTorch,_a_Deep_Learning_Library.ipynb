{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOApXiPQzuDmuxYOfm2X4ib",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohanrathod7/my-ml-labs/blob/main/20_Introduction_to_Deep_Learning_with_PyTorch/01_Introduction_to_PyTorch%2C_a_Deep_Learning_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. TF-IDF and similarity scores\n"
      ],
      "metadata": {
        "id": "69GNV3tpDLy3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "learn how to compute tf-idf weights and the cosine similarity score between two vectors. You will use these concepts to build a movie and a TED Talk recommender. Finally, you will also learn about word embeddings and using word vector representations, you will compute similarities between various Pink Floyd songs."
      ],
      "metadata": {
        "id": "cy5tpla-BBKR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUPpAj0HoP6i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "# Import confusion matrix and train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression, LinearRegression\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/19_Feature_Engineering_for_NLP_in_Python/Dataset/ted.csv\"\n",
        "# Read the CSV file\n",
        "# Apply pd.to_numeric only to relevant columns, excluding 'text'\n",
        "ted = pd.read_csv(url)\n",
        "\n",
        "\n",
        "display(ted.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "u8UlyiPU6rzD",
        "outputId": "40449006-af15-4c8f-b94e-9c15e45b27c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          transcript  \\\n",
              "0  We're going to talk — my — a new lecture, just...   \n",
              "1  This is a representation of your brain, and yo...   \n",
              "2  It's a great honor today to share with you The...   \n",
              "3  My passions are music, technology and making t...   \n",
              "4  It used to be that if you wanted to get a comp...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
              "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
              "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
              "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
              "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0fed5c76-3b22-43bc-b40c-5b32e5bca5dc\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We're going to talk — my — a new lecture, just...</td>\n",
              "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a representation of your brain, and yo...</td>\n",
              "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's a great honor today to share with you The...</td>\n",
              "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My passions are music, technology and making t...</td>\n",
              "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It used to be that if you wanted to get a comp...</td>\n",
              "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0fed5c76-3b22-43bc-b40c-5b32e5bca5dc')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0fed5c76-3b22-43bc-b40c-5b32e5bca5dc button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0fed5c76-3b22-43bc-b40c-5b32e5bca5dc');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e064cac8-1813-4e25-ab70-591f1b65efe3\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e064cac8-1813-4e25-ab70-591f1b65efe3')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e064cac8-1813-4e25-ab70-591f1b65efe3 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(ted\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This is a representation of your brain, and your brain can be broken into two parts. There's the left half, which is the logical side, and then the right half, which is the intuitive. And so if we had a scale to measure the aptitude of each hemisphere, then we can plot our brain. And for example, this would be somebody who's completely logical. This would be someone who's entirely intuitive. So where would you put your brain on this scale? Some of us may have opted for one of these extremes, but I think for most people in the audience, your brain is something like this \\u2014 with a high aptitude in both hemispheres at the same time. It's not like they're mutually exclusive or anything. You can be logical and intuitive.And so I consider myself one of these people, along with most of the other experimental quantum physicists, who need a good deal of logic to string together these complex ideas. But at the same time, we need a good deal of intuition to actually make the experiments work. How do we develop this intuition? Well we like to play with stuff. So we go out and play with it, and then we see how it acts, and then we develop our intuition from there. And really you do the same thing.So some intuition that you may have developed over the years is that one thing is only in one place at a time. I mean, it can sound weird to think about one thing being in two different places at the same time, but you weren't born with this notion, you developed it. And I remember watching a kid playing on a car stop. He was just a toddler and he wasn't very good at it, and he kept falling over. But I bet playing with this car stop taught him a really valuable lesson, and that's that large things don't let you get right past them, and that they stay in one place.And so this is a great conceptual model to have of the world, unless you're a particle physicist. It'd be a terrible model for a particle physicist, because they don't play with car stops, they play with these little weird particles. And when they play with their particles, they find they do all sorts of really weird things \\u2014 like they can fly right through walls, or they can be in two different places at the same time. And so they wrote down all these observations, and they called it the theory of quantum mechanics.And so that's where physics was at a few years ago; you needed quantum mechanics to describe little, tiny particles. But you didn't need it to describe the large, everyday objects around us. This didn't really sit well with my intuition, and maybe it's just because I don't play with particles very often. Well, I play with them sometimes, but not very often. And I've never seen them. I mean, nobody's ever seen a particle. But it didn't sit well with my logical side either. Because if everything is made up of little particles and all the little particles follow quantum mechanics, then shouldn't everything just follow quantum mechanics? I don't see any reason why it shouldn't. And so I'd feel a lot better about the whole thing if we could somehow show that an everyday object also follows quantum mechanics. So a few years ago, I set off to do just that.So I made one. This is the first object that you can see that has been in a mechanical quantum superposition. So what we're looking at here is a tiny computer chip. And you can sort of see this green dot right in the middle. And that's this piece of metal I'm going to be talking about in a minute. This is a photograph of the object. And here I'll zoom in a little bit. We're looking right there in the center. And then here's a really, really big close-up of the little piece of metal. So what we're looking at is a little chunk of metal, and it's shaped like a diving board, and it's sticking out over a ledge. And so I made this thing in nearly the same way as you make a computer chip. I went into a clean room with a fresh silicon wafer, and then I just cranked away at all the big machines for about 100 hours. For the last stuff, I had to build my own machine \\u2014 to make this swimming pool-shaped hole underneath the device. This device has the ability to be in a quantum superposition, but it needs a little help to do it.Here, let me give you an analogy. You know how uncomfortable it is to be in a crowded elevator? I mean, when I'm in an elevator all alone, I do all sorts of weird things, but then other people get on board and I stop doing those things because I don't want to bother them, or, frankly, scare them. So quantum mechanics says that inanimate objects feel the same way. The fellow passengers for inanimate objects are not just people, but it's also the light shining on it and the wind blowing past it and the heat of the room. And so we knew, if we wanted to see this piece of metal behave quantum mechanically, we're going to have to kick out all the other passengers.And so that's what we did. We turned off the lights, and then we put it in a vacuum and sucked out all the air, and then we cooled it down to just a fraction of a degree above absolute zero. Now, all alone in the elevator, the little chunk of metal is free to act however it wanted. And so we measured its motion. We found it was moving in really weird ways. Instead of just sitting perfectly still, it was vibrating, and the way it was vibrating was breathing something like this \\u2014 like expanding and contracting bellows. And by giving it a gentle nudge, we were able to make it both vibrate and not vibrate at the same time \\u2014 something that's only allowed with quantum mechanics.So what I'm telling you here is something truly fantastic. What does it mean for one thing to be both vibrating and not vibrating at the same time? So let's think about the atoms. So in one case: all the trillions of atoms that make up that chunk of metal are sitting still and at the same time those same atoms are moving up and down. Now it's only at precise times when they align. The rest of the time they're delocalized. That means that every atom is in two different places at the same time, which in turn means the entire chunk of metal is in two different places. I think this is really cool. (Laughter) Really.(Applause)It was worth locking myself in a clean room to do this for all those years because, check this out, the difference in scale between a single atom and that chunk of metal is about the same as the difference between that chunk of metal and you. So if a single atom can be in two different places at the same time, that chunk of metal can be in two different places, then why not you? I mean, this is just my logical side talking. So imagine if you're in multiple places at the same time, what would that be like? How would your consciousness handle your body being delocalized in space?There's one more part to the story. It's when we warmed it up, and we turned on the lights and looked inside the box, we saw that the piece metal was still there in one piece. And so I had to develop this new intuition, that it seems like all the objects in the elevator are really just quantum objects just crammed into a tiny space.You hear a lot of talk about how quantum mechanics says that everything is all interconnected. Well, that's not quite right. It's more than that; it's deeper. It's that those connections, your connections to all the things around you, literally define who you are, and that's the profound weirdness of quantum mechanics.Thank you.(Applause)\",\n          \"It used to be that if you wanted to get a computer to do something new, you would have to program it. Now, programming, for those of you here that haven't done it yourself, requires laying out in excruciating detail every single step that you want the computer to do in order to achieve your goal. Now, if you want to do something that you don't know how to do yourself, then this is going to be a great challenge.So this was the challenge faced by this man, Arthur Samuel. In 1956, he wanted to get this computer to be able to beat him at checkers. How can you write a program, lay out in excruciating detail, how to be better than you at checkers? So he came up with an idea: he had the computer play against itself thousands of times and learn how to play checkers. And indeed it worked, and in fact, by 1962, this computer had beaten the Connecticut state champion.So Arthur Samuel was the father of machine learning, and I have a great debt to him, because I am a machine learning practitioner. I was the president of Kaggle, a community of over 200,000 machine learning practictioners. Kaggle puts up competitions to try and get them to solve previously unsolved problems, and it's been successful  hundreds of times. So from this vantage point, I was able to find out a lot about what machine learning can do in the past, can do today, and what it could do in the future. Perhaps the first big success of  machine learning commercially was Google. Google showed that it is possible to find information by using a computer algorithm, and this algorithm is based on machine learning. Since that time, there have been many commercial successes of machine learning. Companies like Amazon and Netflix use machine learning to suggest products that you might like to buy, movies that you might like to watch. Sometimes, it's almost creepy. Companies like LinkedIn and Facebook sometimes will tell you about who your friends might be and you have no idea how it did it, and this is because it's using the power of machine learning. These are algorithms that have learned how to do this from data rather than being programmed by hand.This is also how IBM was successful in getting Watson to beat the two world champions at \\\"Jeopardy,\\\" answering incredibly subtle and complex questions like this one. [\\\"The ancient 'Lion of Nimrud' went missing from this city's national museum in 2003  (along with a lot of other stuff)\\\"] This is also why we are now able to see the first self-driving cars. If you want to be able to tell the difference between, say, a tree and a pedestrian, well, that's pretty important. We don't know how to write those programs by hand, but with machine learning, this is now possible. And in fact, this car has driven  over a million miles without any accidents on regular roads.So we now know that computers can learn, and computers can learn to do things that we actually sometimes don't know how to do ourselves, or maybe can do them better than us. One of the most amazing examples I've seen of machine learning happened on a project that I ran at Kaggle where a team run by a guy called Geoffrey Hinton from the University of Toronto won a competition for automatic drug discovery. Now, what was extraordinary here is not just that they beat all of the algorithms developed by Merck or the international academic community, but nobody on the team had any background in chemistry or biology or life sciences, and they did it in two weeks. How did they do this? They used an extraordinary algorithm called deep learning. So important was this that in fact the success was covered in The New York Times in a front page article a few weeks later. This is Geoffrey Hinton here on the left-hand side. Deep learning is an algorithm inspired by how the human brain works, and as a result it's an algorithm which has no theoretical limitations on what it can do. The more data you give it and the more computation time you give it, the better it gets.The New York Times also showed in this article another extraordinary result of deep learning which I'm going to show you now. It shows that computers  can listen and understand.(Video) Richard Rashid: Now, the last step that I want to be able to take in this process is to actually speak to you in Chinese. Now the key thing there is, we've been able to take a large amount  of information from many Chinese speakers and produce a text-to-speech system that takes Chinese text and converts it into Chinese language, and then we've taken an hour or so of my own voice and we've used that to modulate the standard text-to-speech system so that it would sound like me. Again, the result's not perfect. There are in fact quite a few errors. (In Chinese) (Applause) There's much work to be done in this area. (In Chinese) (Applause)Jeremy Howard: Well, that was at a machine learning conference in China. It's not often, actually, at academic conferences that you do hear spontaneous applause, although of course sometimes at TEDx conferences, feel free. Everything you saw there was happening with deep learning. (Applause) Thank you. The transcription in English was deep learning. The translation to Chinese and the text in the top right, deep learning, and the construction of the voice was deep learning as well.So deep learning is this extraordinary thing. It's a single algorithm that can seem to do almost anything, and I discovered that a year earlier, it had also learned to see. In this obscure competition from Germany called the German Traffic Sign  Recognition Benchmark, deep learning had learned to recognize traffic signs like this one. Not only could it recognize the traffic signs better than any other algorithm, the leaderboard actually showed it was better than people, about twice as good as people. So by 2011, we had the first example of computers that can see better than people. Since that time, a lot has happened. In 2012, Google announced that they had a deep learning algorithm watch YouTube videos and crunched the data on 16,000 computers for a month, and the computer independently learned about concepts such as people and cats just by watching the videos. This is much like the way that humans learn. Humans don't learn by being told what they see, but by learning for themselves what these things are. Also in 2012, Geoffrey Hinton, who we saw earlier, won the very popular ImageNet competition, looking to try to figure out  from one and a half million images what they're pictures of. As of 2014, we're now down to a six percent error rate in image recognition. This is better than people, again.So machines really are doing an extraordinarily good job of this, and it is now being used in industry. For example, Google announced last year that they had mapped every single location in France in two hours, and the way they did it was that they fed street view images into a deep learning algorithm to recognize and read street numbers. Imagine how long it would have taken before: dozens of people, many years. This is also happening in China. Baidu is kind of  the Chinese Google, I guess, and what you see here in the top left is an example of a picture that I uploaded to Baidu's deep learning system, and underneath you can see that the system has understood what that picture is and found similar images. The similar images actually have similar backgrounds, similar directions of the faces, even some with their tongue out. This is not clearly looking at the text of a web page. All I uploaded was an image. So we now have computers which really understand what they see and can therefore search databases of hundreds of millions of images in real time.So what does it mean now that computers can see? Well, it's not just  that computers can see. In fact, deep learning has done more than that. Complex, nuanced sentences like this one are now understandable with deep learning algorithms. As you can see here, this Stanford-based system showing the red dot at the top has figured out that this sentence is expressing negative sentiment. Deep learning now in fact is near human performance at understanding what sentences are about and what it is saying about those things. Also, deep learning has been used to read Chinese, again at about native Chinese speaker level. This algorithm developed out of Switzerland by people, none of whom speak or understand any Chinese. As I say, using deep learning is about the best system in the world for this, even compared to native human understanding.This is a system that we put together at my company which shows putting all this stuff together. These are pictures which have no text attached, and as I'm typing in here sentences, in real time it's understanding these pictures and figuring out what they're about and finding pictures that are similar to the text that I'm writing. So you can see, it's actually understanding my sentences and actually understanding these pictures. I know that you've seen something like this on Google, where you can type in things and it will show you pictures, but actually what it's doing is it's searching the webpage for the text. This is very different from actually understanding the images. This is something that computers have only been able to do for the first time in the last few months.So we can see now that computers can not only see but they can also read, and, of course, we've shown that they can understand what they hear. Perhaps not surprising now that I'm going to tell you they can write. Here is some text that I generated using a deep learning algorithm yesterday. And here is some text that an algorithm out of Stanford generated. Each of these sentences was generated by a deep learning algorithm to describe each of those pictures. This algorithm before has never seen a man in a black shirt playing a guitar. It's seen a man before, it's seen black before, it's seen a guitar before, but it has independently generated this novel description of this picture. We're still not quite at human performance here, but we're close. In tests, humans prefer the computer-generated caption one out of four times. Now this system is now only two weeks old, so probably within the next year, the computer algorithm will be well past human performance at the rate things are going. So computers can also write.So we put all this together and it leads to very exciting opportunities. For example, in medicine, a team in Boston announced that they had discovered dozens of new clinically relevant features of tumors which help doctors make a prognosis of a cancer. Very similarly, in Stanford, a group there announced that, looking at tissues under magnification, they've developed  a machine learning-based system which in fact is better than human pathologists at predicting survival rates for cancer sufferers. In both of these cases, not only were the predictions more accurate, but they generated new insightful science. In the radiology case, they were new clinical indicators that humans can understand. In this pathology case, the computer system actually discovered that the cells around the cancer are as important as the cancer cells themselves in making a diagnosis. This is the opposite of what pathologists had been taught for decades. In each of those two cases, they were systems developed by a combination of medical experts and machine learning experts, but as of last year, we're now beyond that too. This is an example of identifying cancerous areas of human tissue under a microscope. The system being shown here can identify those areas more accurately, or about as accurately, as human pathologists, but was built entirely with deep learning using no medical expertise by people who have no background in the field. Similarly, here, this neuron segmentation. We can now segment neurons about as accurately as humans can, but this system was developed with deep learning using people with no previous  background in medicine.So myself, as somebody with no previous background in medicine, I seem to be entirely well qualified to start a new medical company, which I did. I was kind of terrified of doing it, but the theory seemed to suggest that it ought to be possible to do very useful medicine using just these data analytic techniques. And thankfully, the feedback has been fantastic, not just from the media but from the medical community, who have been very supportive. The theory is that we can take the middle part of the medical process and turn that into data analysis as much as possible, leaving doctors to do what they're best at. I want to give you an example. It now takes us about 15 minutes to generate a new medical diagnostic test and I'll show you that in real time now, but I've compressed it down to  three minutes by cutting some pieces out. Rather than showing you creating a medical diagnostic test, I'm going to show you  a diagnostic test of car images, because that's something we can all understand.So here we're starting with  about 1.5 million car images, and I want to create something that can split them into the angle of the photo that's being taken. So these images are entirely unlabeled, so I have to start from scratch. With our deep learning algorithm, it can automatically identify areas of structure in these images. So the nice thing is that the human and the computer can now work together. So the human, as you can see here, is telling the computer about areas of interest which it wants the computer then to try and use to improve its algorithm. Now, these deep learning systems actually are in 16,000-dimensional space, so you can see here the computer rotating this through that space, trying to find new areas of structure. And when it does so successfully, the human who is driving it can then point out the areas that are interesting. So here, the computer has successfully found areas, for example, angles. So as we go through this process, we're gradually telling the computer more and more about the kinds of structures we're looking for. You can imagine in a diagnostic test this would be a pathologist identifying areas of pathosis, for example, or a radiologist indicating potentially troublesome nodules. And sometimes it can be difficult for the algorithm. In this case, it got kind of confused. The fronts and the backs of the cars are all mixed up. So here we have to be a bit more careful, manually selecting these fronts as opposed to the backs, then telling the computer that this is a type of group that we're interested in.So we do that for a while, we skip over a little bit, and then we train the machine learning algorithm based on these couple of hundred things, and we hope that it's gotten a lot better. You can see, it's now started to fade some of these pictures out, showing us that it already is recognizing how to understand some of these itself. We can then use this concept of similar images, and using similar images, you can now see, the computer at this point is able to entirely find just the fronts of cars. So at this point, the human can tell the computer, okay, yes, you've done a good job of that.Sometimes, of course, even at this point it's still difficult to separate out groups. In this case, even after we let the computer try to rotate this for a while, we still find that the left sides and the right sides pictures are all mixed up together. So we can again give the computer some hints, and we say, okay, try and find a projection that separates out the left sides and the right sides as much as possible using this deep learning algorithm. And giving it that hint \\u2014 ah, okay, it's been successful. It's managed to find a way of thinking about these objects that's separated out these together.So you get the idea here. This is a case not where the human is being replaced by a computer, but where they're working together. What we're doing here is we're replacing something that used to take a team of five or six people about seven years and replacing it with something that takes 15 minutes for one person acting alone.So this process takes about four or five iterations. You can see we now have 62 percent of our 1.5 million images  classified correctly. And at this point, we can start to quite quickly grab whole big sections, check through them to make sure that there's no mistakes. Where there are mistakes, we can let the computer know about them. And using this kind of process for each of the different groups, we are now up to an 80 percent success rate in classifying the 1.5 million images. And at this point, it's just a case of finding the small number that aren't classified correctly, and trying to understand why. And using that approach, by 15 minutes we get to 97 percent classification rates.So this kind of technique could allow us to fix a major problem, which is that there's a lack of medical expertise in the world. The World Economic Forum says that there's between a 10x and a 20x shortage of physicians in the developing world, and it would take about 300 years to train enough people to fix that problem. So imagine if we can help enhance their efficiency using these deep learning approaches?So I'm very excited about the opportunities. I'm also concerned about the problems. The problem here is that every area in blue on this map is somewhere where services are over 80 percent of employment. What are services? These are services. These are also the exact things that computers have just learned how to do. So 80 percent of the world's employment in the developed world is stuff that computers  have just learned how to do. What does that mean? Well, it'll be fine. They'll be replaced by other jobs. For example, there will be more jobs for data scientists. Well, not really. It doesn't take data scientists  very long to build these things. For example, these four algorithms were all built by the same guy. So if you think, oh,  it's all happened before, we've seen the results in the past of when new things come along and they get replaced by new jobs, what are these new jobs going to be? It's very hard for us to estimate this, because human performance grows at this gradual rate, but we now have a system, deep learning, that we know actually grows in capability exponentially. And we're here. So currently, we see the things around us and we say, \\\"Oh, computers are still pretty dumb.\\\" Right? But in five years' time, computers will be off this chart. So we need to be starting to think about this capability right now.We have seen this once before, of course. In the Industrial Revolution, we saw a step change in capability thanks to engines. The thing is, though, that after a while, things flattened out. There was social disruption, but once engines were used  to generate power in all the situations, things really settled down. The Machine Learning Revolution is going to be very different from the Industrial Revolution, because the Machine Learning Revolution, it never settles down. The better computers get at intellectual activities, the more they can build better computers to be better at intellectual capabilities, so this is going to be a kind of change that the world has actually never experienced before, so your previous understanding of what's possible is different.This is already impacting us. In the last 25 years, as capital productivity has increased, labor productivity has been flat, in fact even a little bit down.So I want us to start having this discussion now. I know that when I often tell people about this situation, people can be quite dismissive. Well, computers can't really think, they don't emote, they don't understand poetry, we don't really understand how they work. So what? Computers right now can do the things that humans spend most of their time being paid to do, so now's the time to start thinking about how we're going to adjust our social structures and economic structures to be aware of this new reality. Thank you. (Applause)\",\n          \"It's a great honor today to share with you The Digital Universe, which was created for humanity to really see where we are in the universe. And so I think we can roll the video that we have.[The Himalayas.](Music)The flat horizon that we've evolved with has been a metaphor for the infinite: unbounded resources and unlimited capacity for disposal of waste. It wasn't until we really left Earth, got above the atmosphere and had seen the horizon bend back on itself, that we could understand our planet as a limited condition. The Digital Universe Atlas has been built at the American Museum of Natural History over the past 12 years. We maintain that, put that together as a project to really chart the universe across all scales. What we see here are satellites around the Earth and the Earth in proper registration against the universe, as we see. NASA supported this work 12 years ago as part of the rebuilding of the Hayden Planetarium so that we would share this with the world.The Digital Universe is the basis of our space show productions that we do \\u2014 our main space shows in the dome. But what you see here is the result of, actually, internships that we hosted with Linkoping University in Sweden. I've had 12 students work on this for their graduate work, and the result has been this software called Uniview and a company called SCISS in Sweden. This software allows interactive use, so this actual flight path and movie that we see here was actually flown live. I captured this live from my laptop in a cafe called Earth Matters on the Lower East Side of Manhattan, where I live, and it was done as a collaborative project with the Rubin Museum of Himalayan Art for an exhibit on comparative cosmology.And so as we move out, we see continuously from our planet all the way out into the realm of galaxies, as we see here, light-travel time, giving you a sense of how far away we are. As we move out, the light from these distant galaxies have taken so long, we're essentially backing up into the past. We back so far up we're finally seeing a containment around us \\u2014 the afterglow of the Big Bang. This is the WMAP microwave background that we see. We'll fly outside it here, just to see this sort of containment. If we were outside this, it would almost be meaningless, in the sense as before time. But this our containment of the visible universe. We know the universe is bigger than that which we can see.Coming back quickly, we see here the radio sphere that we jumped out of in the beginning, but these are positions, the latest positions of exoplanets that we've mapped, and our sun here, obviously, with our own solar system. What you're going to see \\u2014 we're going to have to jump in here pretty quickly between several orders of magnitude to get down to where we see the solar system \\u2014 these are the paths of Voyager 1, Voyager 2, Pioneer 11 and Pioneer 10, the first four spacecraft to have left the solar system. Coming in closer, picking up Earth, orbit of the Moon, and we see the Earth. This map can be updated, and we can add in new data.I know Dr. Carolyn Porco is the camera P.I. for the Cassini mission. But here we see the complex trajectory of the Cassini mission color coded for different mission phases, ingeniously developed so that 45 encounters with the largest moon, Titan, which is larger that the planet Mercury, diverts the orbit into different parts of mission phase.This software allows us to come close and look at parts of this. This software can also be networked between domes. We have a growing user base of this, and we network domes. And we can network between domes and classrooms. We're actually sharing tours of the universe with the first sub-Saharan planetarium in Ghana as well as new libraries that have been built in the ghettos in Columbia and a high school in Cambodia. And the Cambodians have actually controlled the Hayden Planetarium from their high school.This is an image from Saturday, photographed by the Aqua satellite, but through the Uniview software. So you're seeing the edge of the Earth. This is Nepal. This is, in fact, right here is the valley of Lhasa, right here in Tibet. But we can see the haze from fires and so forth in the Ganges valley down below in India. This is Nepal and Tibet.And just in closing, I'd just like to say this beautiful world that we live on \\u2014 here we see a bit of the snow that some of you may have had to brave in coming out \\u2014 so I'd like to just say that what the world needs now is a sense of being able to look at ourselves in this much larger condition now and a much larger sense of what home is. Because our home is the universe, and we are the universe, essentially. We carry that in us. And to be able to see our context in this larger sense at all scales helps us all, I think, in understanding where we are and who we are in the universe.Thank you.(Applause)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://www.ted.com/talks/aaron_o_connell_making_sense_of_a_visible_quantum_object\\n\",\n          \"https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn\\n\",\n          \"https://www.ted.com/talks/carter_emmart_demos_a_3d_atlas_of_the_universe\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3NcmfkC7C9B4",
        "outputId": "d116a482-066f-402c-de08-9994bd0e0ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/pip': [Errno 2] No such file or directory\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load en_core_web_sm and create an nlp object\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "YgYaAX_wC_X3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tf-idf weight of commonly occurring words**  \n",
        "The word bottle occurs 5 times in a particular document D and also occurs in every document of the corpus. What is the tf-idf weight of bottle in D?\n",
        "\n",
        "    -> 0\n",
        "\n",
        "    In fact, the tf-idf weight for bottle in every document will be 0. This is because the inverse document frequency is constant across documents in a corpus and since bottle occurs in every document, its value is log(1), which is 0."
      ],
      "metadata": {
        "id": "kamCKJTwK2Mj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**tf-idf vectors for TED talks**  \n",
        "In this exercise, you have been given a corpus ted which contains the transcripts of 500 TED Talks. Your task is to generate the tf-idf vectors for these talks.\n",
        "\n",
        "In a later lesson, we will use these vectors to generate recommendations of similar talks based on the transcript."
      ],
      "metadata": {
        "id": "AOexsfG3LagW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Create TfidfVectorizer object\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Generate matrix of word vectors\n",
        "tfidf_matrix = vectorizer.fit_transform(ted)\n",
        "\n",
        "# Print the shape of tfidf_matrix\n",
        "print(tfidf_matrix.shape)\n",
        "\n",
        "#  You now know how to generate tf-idf vectors for a given corpus of text.\n",
        "#  You can use these vectors to perform predictive modeling just like we did with CountVectorizer.\n",
        "#   In the next few lessons, we will see another extremely useful application of the vectorized form of documents: generating recommendations."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pIKZtXjDLhpn",
        "outputId": "05c523cb-acbf-4a7f-c76f-8530843d28c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cosine similarity"
      ],
      "metadata": {
        "id": "vJ4yP8EzMJZh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Range of cosine scores**  \n",
        "Which of the following is a possible cosine score for a pair of document vectors?\n",
        "\n",
        "    -> 0.84\n",
        "\n",
        "    Since document vectors use only non-negative weights, the cosine score lies between 0 and 1."
      ],
      "metadata": {
        "id": "hoIaBMcENDop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing dot product**  \n",
        "In this exercise, we will learn to compute the dot product between two vectors, A = (1, 3) and B = (-2, 2), using the numpy library. More specifically, we will use the np.dot() function to compute the dot product of two numpy arrays."
      ],
      "metadata": {
        "id": "Lgh1FUL8NP44"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize numpy vectors\n",
        "A = np.array([1,3])\n",
        "B = np.array([-2,2])\n",
        "\n",
        "# Compute dot product\n",
        "dot_prod = np.dot(A, B)\n",
        "\n",
        "# Print dot product\n",
        "print(dot_prod)\n",
        "\n",
        "# The dot product of the two vectors is 1 * -2 + 3 * 2 = 4, which is indeed the output produced.\n",
        "# We will not be using np.dot() too much in this course but it can prove to be a helpful function\n",
        "# while computing dot products between two standalone vectors."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eH5jHKyCNfYk",
        "outputId": "3e7953c4-95b6-45c4-dcf2-65c5b77c1b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Cosine similarity matrix of a corpus***  \n",
        "In this exercise, you have been given a corpus, which is a list containing five sentences. The corpus is printed in the console. You have to compute the cosine similarity matrix which contains the pairwise cosine similarity score for every pair of sentences (vectorized using tf-idf).\n",
        "\n",
        "Remember, the value corresponding to the ith row and jth column of a similarity matrix denotes the similarity score for the ith and jth vector."
      ],
      "metadata": {
        "id": "fAvcfI63Nlw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import cosine_similarity\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "# Initialize an instance of tf-idf Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Import CountVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/19_Feature_Engineering_for_NLP_in_Python/Dataset/movie_overviews.csv\"\n",
        "# Read the CSV file\n",
        "corpus = ['The sun is the largest celestial body in the solar system', 'The solar system consists of the sun and eight revolving planets', 'Ra was the Egyptian Sun God', 'The Pyramids were the pinnacle of Egyptian architecture', 'The quick brown fox jumps over the lazy dog']\n",
        "\n",
        "\n",
        "\n",
        "# Fill missing values in 'tagline' with empty strings\n",
        "\n",
        "\n",
        "\n",
        "# Generate the tf-idf vectors for the corpus\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Compute and print the cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_sim)\n",
        "\n",
        "# As you will see in a subsequent lesson, computing the cosine similarity matrix lies at the heart of many practical systems such as recommenders.\n",
        "# From our similarity matrix, we see that the first and the second sentence are the most similar.\n",
        "# Also the fifth sentence has, on average, the lowest pairwise cosine scores.\n",
        "# This is intuitive as it contains entities that are not present in the other sentences."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B17CzkaXNxWC",
        "outputId": "449ffabd-fa56-4944-9187-fa5345c0f34f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a plot line based recommender"
      ],
      "metadata": {
        "id": "mW6BO4AkOtFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Comparing linear_kernel and cosine_similarity**  \n",
        "In this exercise, you have been given tfidf_matrix which contains the tf-idf vectors of a thousand documents. Your task is to generate the cosine similarity matrix for these vectors first using cosine_similarity and then, using linear_kernel.\n",
        "\n",
        "We will then compare the computation times for both functions."
      ],
      "metadata": {
        "id": "dOmr67mfPctS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Compute the cosine similarity matrix for tfidf_matrix using cosine_similarity.***"
      ],
      "metadata": {
        "id": "Qn84nii2PnkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV8spMBKPj_W",
        "outputId": "d20cfd5d-c5cc-4530-fccc-2ff107cab6f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
            "Time taken: 0.0061833858489990234 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "# Record start time\n",
        "start = time.time()\n",
        "\n",
        "# Compute cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Print cosine similarity matrix\n",
        "print(cosine_sim)\n",
        "\n",
        "# Print time taken\n",
        "print(\"Time taken: %s seconds\" %(time.time() - start))\n",
        "\n",
        "# Notice how both linear_kernel and cosine_similarity produced the same result.\n",
        "# However, linear_kernel took a smaller amount of time to execute.\n",
        "# When you're working with a very large amount of data and your vectors are in the tf-idf representation,\n",
        "# it is good practice to default to linear_kernel to improve performance.\n",
        "#  (NOTE: In case, you see linear_kernel taking more time, it's because the dataset we're dealing with is\n",
        "#   extremely small and Python's time module is incapable of capture such minute time differences accurately)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AvQmKuvsPx4q",
        "outputId": "9c828109-0d99-496a-ce4d-4b0b5191b903"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1.         0.36413198 0.18314713 0.18435251 0.16336438]\n",
            " [0.36413198 1.         0.15054075 0.21704584 0.11203887]\n",
            " [0.18314713 0.15054075 1.         0.21318602 0.07763512]\n",
            " [0.18435251 0.21704584 0.21318602 1.         0.12960089]\n",
            " [0.16336438 0.11203887 0.07763512 0.12960089 1.        ]]\n",
            "Time taken: 0.0013508796691894531 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Plot recommendation engine**  \n",
        "In this exercise, we will build a recommendation engine that suggests movies based on similarity of plot lines. You have been given a get_recommendations() function that takes in the title of a movie, a similarity matrix and an indices series as its arguments and outputs a list of most similar movies. indices has already been provided to you.\n",
        "\n",
        "You have also been given a movie_plots Series that contains the plot lines of several movies. Your task is to generate a cosine similarity matrix for the tf-idf vectors of these plots.\n",
        "\n",
        "Consequently, we will check the potency of our engine by generating recommendations for one of my favorite movies, The Dark Knight Rises."
      ],
      "metadata": {
        "id": "9cIRe1pIQLMv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/19_Feature_Engineering_for_NLP_in_Python/Dataset/movie_overviews.csv\"\n",
        "# Read the CSV file\n",
        "movie_plots = pd.read_csv(url)\n",
        "\n",
        "# Fill missing values in 'overview' with empty strings\n",
        "movie_plots['overview'] = movie_plots['overview'].fillna('')\n",
        "\n",
        "display(movie_plots.head())\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(movie_plots[\"overview\"])\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Placeholder for indices - replace with your actual indices Series\n",
        "indices = pd.Series(movie_plots.index, index=movie_plots['title']).drop_duplicates()\n",
        "\n",
        "# Placeholder for get_recommendations function - replace with your actual function\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # Get the index of the movie that matches the title\n",
        "    idx = indices[title]\n",
        "    # Get the pairwsie similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return movie_plots['title'].iloc[movie_indices]\n",
        "\n",
        "# Generate recommendations\n",
        "print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\n",
        "\n",
        "# You've just built your very first recommendation system. Notice how the recommender correctly identifies\n",
        "# 'The Dark Knight Rises' as a Batman movie and recommends other Batman movies as a result. This sytem is,\n",
        "# of course, very primitive and there are a host of ways in which it could be improved. One method would be\n",
        "# to look at the cast, crew and genre in addition to the plot to generate recommendations. We will not be covering this\n",
        "# in this course but you have all the tools necessary to accomplish this. Do give it a try!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "mkYq8xTkQK-y",
        "outputId": "0cc8b7b8-f5e3-4a1a-8405-57eae1a61e5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "      id                        title  \\\n",
              "0    862                    Toy Story   \n",
              "1   8844                      Jumanji   \n",
              "2  15602             Grumpier Old Men   \n",
              "3  31357            Waiting to Exhale   \n",
              "4  11862  Father of the Bride Part II   \n",
              "\n",
              "                                            overview  \\\n",
              "0  Led by Woody, Andy's toys live happily in his ...   \n",
              "1  When siblings Judy and Peter discover an encha...   \n",
              "2  A family wedding reignites the ancient feud be...   \n",
              "3  Cheated on, mistreated and stepped on, the wom...   \n",
              "4  Just when George Banks has recovered from his ...   \n",
              "\n",
              "                                             tagline  \n",
              "0                                                NaN  \n",
              "1          Roll the dice and unleash the excitement!  \n",
              "2  Still Yelling. Still Fighting. Still Ready for...  \n",
              "3  Friends are the people who let you be yourself...  \n",
              "4  Just When His World Is Back To Normal... He's ...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f83ae70b-ac93-463b-8408-37115f17a93f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>title</th>\n",
              "      <th>overview</th>\n",
              "      <th>tagline</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>862</td>\n",
              "      <td>Toy Story</td>\n",
              "      <td>Led by Woody, Andy's toys live happily in his ...</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8844</td>\n",
              "      <td>Jumanji</td>\n",
              "      <td>When siblings Judy and Peter discover an encha...</td>\n",
              "      <td>Roll the dice and unleash the excitement!</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>15602</td>\n",
              "      <td>Grumpier Old Men</td>\n",
              "      <td>A family wedding reignites the ancient feud be...</td>\n",
              "      <td>Still Yelling. Still Fighting. Still Ready for...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>31357</td>\n",
              "      <td>Waiting to Exhale</td>\n",
              "      <td>Cheated on, mistreated and stepped on, the wom...</td>\n",
              "      <td>Friends are the people who let you be yourself...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>11862</td>\n",
              "      <td>Father of the Bride Part II</td>\n",
              "      <td>Just when George Banks has recovered from his ...</td>\n",
              "      <td>Just When His World Is Back To Normal... He's ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f83ae70b-ac93-463b-8408-37115f17a93f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f83ae70b-ac93-463b-8408-37115f17a93f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f83ae70b-ac93-463b-8408-37115f17a93f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-6d06bac1-ad6e-45ba-95a4-d0233398b86f\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6d06bac1-ad6e-45ba-95a4-d0233398b86f')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-6d06bac1-ad6e-45ba-95a4-d0233398b86f button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(get_recommendations('The Dark Knight Rises', cosine_sim, indices))\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 11260,\n        \"min\": 862,\n        \"max\": 31357,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          8844,\n          11862,\n          15602\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"Jumanji\",\n          \"Father of the Bride Part II\",\n          \"Grumpier Old Men\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"overview\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"When siblings Judy and Peter discover an enchanted board game that opens the door to a magical world, they unwittingly invite Alan -- an adult who's been trapped inside the game for 26 years -- into their living room. Alan's only hope for freedom is to finish the game, which proves risky as all three find themselves running from giant rhinoceroses, evil monkeys and other terrifying creatures.\",\n          \"Just when George Banks has recovered from his daughter's wedding, he receives the news that she's pregnant ... and that George's wife, Nina, is expecting too. He was planning on selling their home, but that's a plan that -- like George -- will have to change with the arrival of both a grandchild and a kid of his own.\",\n          \"A family wedding reignites the ancient feud between next-door neighbors and fishing buddies John and Max. Meanwhile, a sultry Italian divorc\\u00e9e opens a restaurant at the local bait shop, alarming the locals who worry she'll scare the fish away. But she's less interested in seafood than she is in cooking up a hot time with Max.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tagline\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"Still Yelling. Still Fighting. Still Ready for Love.\",\n          \"Just When His World Is Back To Normal... He's In For The Surprise Of His Life!\",\n          \"Roll the dice and unleash the excitement!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "132                              Batman Forever\n",
            "6907                            The Dark Knight\n",
            "1116                             Batman Returns\n",
            "7573                 Batman: Under the Red Hood\n",
            "524                                      Batman\n",
            "7907                           Batman: Year One\n",
            "8171    Batman: The Dark Knight Returns, Part 1\n",
            "2581               Batman: Mask of the Phantasm\n",
            "8232    Batman: The Dark Knight Returns, Part 2\n",
            "6150                              Batman Begins\n",
            "Name: title, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The recommender function**  \n",
        "In this exercise, we will build a recommender function get_recommendations(), as discussed in the lesson and the previous exercise. As we know, it takes in a title, a cosine similarity matrix, and a movie title and index mapping as arguments and outputs a list of 10 titles most similar to the original title (excluding the title itself).\n",
        "\n",
        "You have been given a dataset metadata that consists of the movie titles and overviews. The head of this dataset has been printed to console."
      ],
      "metadata": {
        "id": "sEEtwasER7IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate mapping between titles and index\n",
        "indices = pd.Series(movie_plots.index, index=movie_plots['title']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(title, cosine_sim, indices):\n",
        "    # To get the index of the movie that matches the title\n",
        "    idx = indices[title]\n",
        "    # Sort the movies based on the similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar movies\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the movie indices\n",
        "    movie_indices = [i[0] for i in sim_scores]\n",
        "    # Return the top 10 most similar movies\n",
        "    return movie_plots['title'].iloc[movie_indices]\n",
        "\n",
        "    # With this recommender function in our toolkit, we are now in a very good place to build the rest of the components of our recommendation engine."
      ],
      "metadata": {
        "id": "mdxCWXstSFkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TED talk recommender**  \n",
        "In this exercise, we will build a recommendation system that suggests TED Talks based on their transcripts. You have been given a get_recommendations() function that takes in the title of a talk, a similarity matrix and an indices series as its arguments, and outputs a list of most similar talks. indices has already been provided to you.\n",
        "\n",
        "You have also been given a transcripts series that contains the transcripts of around 500 TED talks. Your task is to generate a cosine similarity matrix for the tf-idf vectors of the talk transcripts."
      ],
      "metadata": {
        "id": "RADiMoMkSfZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/19_Feature_Engineering_for_NLP_in_Python/Dataset/ted.csv\"\n",
        "# Read the CSV file\n",
        "ted = pd.read_csv(url)\n",
        "\n",
        "display(ted.head())\n",
        "\n",
        "# Initialize the TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(stop_words='english')\n",
        "\n",
        "# Construct the TF-IDF matrix\n",
        "tfidf_matrix = tfidf.fit_transform(ted[\"transcript\"])\n",
        "\n",
        "# Generate the cosine similarity matrix\n",
        "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Generate mapping between URLs and index\n",
        "indices = pd.Series(ted.index, index=ted['url']).drop_duplicates()\n",
        "\n",
        "def get_recommendations(talk_url, cosine_sim, indices):\n",
        "    # Get the index of the talk that matches the URL\n",
        "    try:\n",
        "        idx = indices[talk_url]\n",
        "    except KeyError:\n",
        "        return f\"Talk with URL '{talk_url}' not found in the dataset indices.\"\n",
        "\n",
        "    # Get the pairwise similarity scores\n",
        "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
        "    # Sort the talks based on the similarity scores\n",
        "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "    # Get the scores for 10 most similar talks (excluding the talk itself)\n",
        "    sim_scores = sim_scores[1:11]\n",
        "    # Get the talk indices\n",
        "    talk_indices = [i[0] for i in sim_scores]\n",
        "    # Return the URLs of the top 10 most similar talks\n",
        "    return ted['url'].iloc[talk_indices]\n",
        "\n",
        "\n",
        "# Generate recommendations for the first talk in the dataset as an example\n",
        "example_talk_url = ted['url'].iloc[0]\n",
        "print(f\"Recommendations for talk at URL: {example_talk_url}\")\n",
        "print(get_recommendations(example_talk_url, cosine_sim, indices))\n",
        "\n",
        "# If you want recommendations for a specific talk like '5 ways to kill your dreams', you would need its corresponding URL from the dataset.\n",
        "# For example, if the URL for '5 ways to kill your dreams' is 'https://www.ted.com/talks/xyz_5_ways_to_kill_your_dreams', you would call:\n",
        "# print(get_recommendations('https://www.ted.com/talks/xyz_5_ways_to_kill_your_dreams', cosine_sim, indices))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        },
        "id": "-cvTF0z8Sgpi",
        "outputId": "96e15071-1f1a-4a43-9578-42f245ed86fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "                                          transcript  \\\n",
              "0  We're going to talk — my — a new lecture, just...   \n",
              "1  This is a representation of your brain, and yo...   \n",
              "2  It's a great honor today to share with you The...   \n",
              "3  My passions are music, technology and making t...   \n",
              "4  It used to be that if you wanted to get a comp...   \n",
              "\n",
              "                                                 url  \n",
              "0  https://www.ted.com/talks/al_seckel_says_our_b...  \n",
              "1  https://www.ted.com/talks/aaron_o_connell_maki...  \n",
              "2  https://www.ted.com/talks/carter_emmart_demos_...  \n",
              "3  https://www.ted.com/talks/jared_ficklin_new_wa...  \n",
              "4  https://www.ted.com/talks/jeremy_howard_the_wo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-195b3cc2-71b5-4916-91d5-8bf834c7016b\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>transcript</th>\n",
              "      <th>url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>We're going to talk — my — a new lecture, just...</td>\n",
              "      <td>https://www.ted.com/talks/al_seckel_says_our_b...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is a representation of your brain, and yo...</td>\n",
              "      <td>https://www.ted.com/talks/aaron_o_connell_maki...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>It's a great honor today to share with you The...</td>\n",
              "      <td>https://www.ted.com/talks/carter_emmart_demos_...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>My passions are music, technology and making t...</td>\n",
              "      <td>https://www.ted.com/talks/jared_ficklin_new_wa...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>It used to be that if you wanted to get a comp...</td>\n",
              "      <td>https://www.ted.com/talks/jeremy_howard_the_wo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-195b3cc2-71b5-4916-91d5-8bf834c7016b')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-195b3cc2-71b5-4916-91d5-8bf834c7016b button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-195b3cc2-71b5-4916-91d5-8bf834c7016b');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-67437b6c-0808-4000-bf64-769cbc4c43d1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-67437b6c-0808-4000-bf64-769cbc4c43d1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-67437b6c-0808-4000-bf64-769cbc4c43d1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# print(get_recommendations('https://www\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"transcript\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"This is a representation of your brain, and your brain can be broken into two parts. There's the left half, which is the logical side, and then the right half, which is the intuitive. And so if we had a scale to measure the aptitude of each hemisphere, then we can plot our brain. And for example, this would be somebody who's completely logical. This would be someone who's entirely intuitive. So where would you put your brain on this scale? Some of us may have opted for one of these extremes, but I think for most people in the audience, your brain is something like this \\u2014 with a high aptitude in both hemispheres at the same time. It's not like they're mutually exclusive or anything. You can be logical and intuitive.And so I consider myself one of these people, along with most of the other experimental quantum physicists, who need a good deal of logic to string together these complex ideas. But at the same time, we need a good deal of intuition to actually make the experiments work. How do we develop this intuition? Well we like to play with stuff. So we go out and play with it, and then we see how it acts, and then we develop our intuition from there. And really you do the same thing.So some intuition that you may have developed over the years is that one thing is only in one place at a time. I mean, it can sound weird to think about one thing being in two different places at the same time, but you weren't born with this notion, you developed it. And I remember watching a kid playing on a car stop. He was just a toddler and he wasn't very good at it, and he kept falling over. But I bet playing with this car stop taught him a really valuable lesson, and that's that large things don't let you get right past them, and that they stay in one place.And so this is a great conceptual model to have of the world, unless you're a particle physicist. It'd be a terrible model for a particle physicist, because they don't play with car stops, they play with these little weird particles. And when they play with their particles, they find they do all sorts of really weird things \\u2014 like they can fly right through walls, or they can be in two different places at the same time. And so they wrote down all these observations, and they called it the theory of quantum mechanics.And so that's where physics was at a few years ago; you needed quantum mechanics to describe little, tiny particles. But you didn't need it to describe the large, everyday objects around us. This didn't really sit well with my intuition, and maybe it's just because I don't play with particles very often. Well, I play with them sometimes, but not very often. And I've never seen them. I mean, nobody's ever seen a particle. But it didn't sit well with my logical side either. Because if everything is made up of little particles and all the little particles follow quantum mechanics, then shouldn't everything just follow quantum mechanics? I don't see any reason why it shouldn't. And so I'd feel a lot better about the whole thing if we could somehow show that an everyday object also follows quantum mechanics. So a few years ago, I set off to do just that.So I made one. This is the first object that you can see that has been in a mechanical quantum superposition. So what we're looking at here is a tiny computer chip. And you can sort of see this green dot right in the middle. And that's this piece of metal I'm going to be talking about in a minute. This is a photograph of the object. And here I'll zoom in a little bit. We're looking right there in the center. And then here's a really, really big close-up of the little piece of metal. So what we're looking at is a little chunk of metal, and it's shaped like a diving board, and it's sticking out over a ledge. And so I made this thing in nearly the same way as you make a computer chip. I went into a clean room with a fresh silicon wafer, and then I just cranked away at all the big machines for about 100 hours. For the last stuff, I had to build my own machine \\u2014 to make this swimming pool-shaped hole underneath the device. This device has the ability to be in a quantum superposition, but it needs a little help to do it.Here, let me give you an analogy. You know how uncomfortable it is to be in a crowded elevator? I mean, when I'm in an elevator all alone, I do all sorts of weird things, but then other people get on board and I stop doing those things because I don't want to bother them, or, frankly, scare them. So quantum mechanics says that inanimate objects feel the same way. The fellow passengers for inanimate objects are not just people, but it's also the light shining on it and the wind blowing past it and the heat of the room. And so we knew, if we wanted to see this piece of metal behave quantum mechanically, we're going to have to kick out all the other passengers.And so that's what we did. We turned off the lights, and then we put it in a vacuum and sucked out all the air, and then we cooled it down to just a fraction of a degree above absolute zero. Now, all alone in the elevator, the little chunk of metal is free to act however it wanted. And so we measured its motion. We found it was moving in really weird ways. Instead of just sitting perfectly still, it was vibrating, and the way it was vibrating was breathing something like this \\u2014 like expanding and contracting bellows. And by giving it a gentle nudge, we were able to make it both vibrate and not vibrate at the same time \\u2014 something that's only allowed with quantum mechanics.So what I'm telling you here is something truly fantastic. What does it mean for one thing to be both vibrating and not vibrating at the same time? So let's think about the atoms. So in one case: all the trillions of atoms that make up that chunk of metal are sitting still and at the same time those same atoms are moving up and down. Now it's only at precise times when they align. The rest of the time they're delocalized. That means that every atom is in two different places at the same time, which in turn means the entire chunk of metal is in two different places. I think this is really cool. (Laughter) Really.(Applause)It was worth locking myself in a clean room to do this for all those years because, check this out, the difference in scale between a single atom and that chunk of metal is about the same as the difference between that chunk of metal and you. So if a single atom can be in two different places at the same time, that chunk of metal can be in two different places, then why not you? I mean, this is just my logical side talking. So imagine if you're in multiple places at the same time, what would that be like? How would your consciousness handle your body being delocalized in space?There's one more part to the story. It's when we warmed it up, and we turned on the lights and looked inside the box, we saw that the piece metal was still there in one piece. And so I had to develop this new intuition, that it seems like all the objects in the elevator are really just quantum objects just crammed into a tiny space.You hear a lot of talk about how quantum mechanics says that everything is all interconnected. Well, that's not quite right. It's more than that; it's deeper. It's that those connections, your connections to all the things around you, literally define who you are, and that's the profound weirdness of quantum mechanics.Thank you.(Applause)\",\n          \"It used to be that if you wanted to get a computer to do something new, you would have to program it. Now, programming, for those of you here that haven't done it yourself, requires laying out in excruciating detail every single step that you want the computer to do in order to achieve your goal. Now, if you want to do something that you don't know how to do yourself, then this is going to be a great challenge.So this was the challenge faced by this man, Arthur Samuel. In 1956, he wanted to get this computer to be able to beat him at checkers. How can you write a program, lay out in excruciating detail, how to be better than you at checkers? So he came up with an idea: he had the computer play against itself thousands of times and learn how to play checkers. And indeed it worked, and in fact, by 1962, this computer had beaten the Connecticut state champion.So Arthur Samuel was the father of machine learning, and I have a great debt to him, because I am a machine learning practitioner. I was the president of Kaggle, a community of over 200,000 machine learning practictioners. Kaggle puts up competitions to try and get them to solve previously unsolved problems, and it's been successful  hundreds of times. So from this vantage point, I was able to find out a lot about what machine learning can do in the past, can do today, and what it could do in the future. Perhaps the first big success of  machine learning commercially was Google. Google showed that it is possible to find information by using a computer algorithm, and this algorithm is based on machine learning. Since that time, there have been many commercial successes of machine learning. Companies like Amazon and Netflix use machine learning to suggest products that you might like to buy, movies that you might like to watch. Sometimes, it's almost creepy. Companies like LinkedIn and Facebook sometimes will tell you about who your friends might be and you have no idea how it did it, and this is because it's using the power of machine learning. These are algorithms that have learned how to do this from data rather than being programmed by hand.This is also how IBM was successful in getting Watson to beat the two world champions at \\\"Jeopardy,\\\" answering incredibly subtle and complex questions like this one. [\\\"The ancient 'Lion of Nimrud' went missing from this city's national museum in 2003  (along with a lot of other stuff)\\\"] This is also why we are now able to see the first self-driving cars. If you want to be able to tell the difference between, say, a tree and a pedestrian, well, that's pretty important. We don't know how to write those programs by hand, but with machine learning, this is now possible. And in fact, this car has driven  over a million miles without any accidents on regular roads.So we now know that computers can learn, and computers can learn to do things that we actually sometimes don't know how to do ourselves, or maybe can do them better than us. One of the most amazing examples I've seen of machine learning happened on a project that I ran at Kaggle where a team run by a guy called Geoffrey Hinton from the University of Toronto won a competition for automatic drug discovery. Now, what was extraordinary here is not just that they beat all of the algorithms developed by Merck or the international academic community, but nobody on the team had any background in chemistry or biology or life sciences, and they did it in two weeks. How did they do this? They used an extraordinary algorithm called deep learning. So important was this that in fact the success was covered in The New York Times in a front page article a few weeks later. This is Geoffrey Hinton here on the left-hand side. Deep learning is an algorithm inspired by how the human brain works, and as a result it's an algorithm which has no theoretical limitations on what it can do. The more data you give it and the more computation time you give it, the better it gets.The New York Times also showed in this article another extraordinary result of deep learning which I'm going to show you now. It shows that computers  can listen and understand.(Video) Richard Rashid: Now, the last step that I want to be able to take in this process is to actually speak to you in Chinese. Now the key thing there is, we've been able to take a large amount  of information from many Chinese speakers and produce a text-to-speech system that takes Chinese text and converts it into Chinese language, and then we've taken an hour or so of my own voice and we've used that to modulate the standard text-to-speech system so that it would sound like me. Again, the result's not perfect. There are in fact quite a few errors. (In Chinese) (Applause) There's much work to be done in this area. (In Chinese) (Applause)Jeremy Howard: Well, that was at a machine learning conference in China. It's not often, actually, at academic conferences that you do hear spontaneous applause, although of course sometimes at TEDx conferences, feel free. Everything you saw there was happening with deep learning. (Applause) Thank you. The transcription in English was deep learning. The translation to Chinese and the text in the top right, deep learning, and the construction of the voice was deep learning as well.So deep learning is this extraordinary thing. It's a single algorithm that can seem to do almost anything, and I discovered that a year earlier, it had also learned to see. In this obscure competition from Germany called the German Traffic Sign  Recognition Benchmark, deep learning had learned to recognize traffic signs like this one. Not only could it recognize the traffic signs better than any other algorithm, the leaderboard actually showed it was better than people, about twice as good as people. So by 2011, we had the first example of computers that can see better than people. Since that time, a lot has happened. In 2012, Google announced that they had a deep learning algorithm watch YouTube videos and crunched the data on 16,000 computers for a month, and the computer independently learned about concepts such as people and cats just by watching the videos. This is much like the way that humans learn. Humans don't learn by being told what they see, but by learning for themselves what these things are. Also in 2012, Geoffrey Hinton, who we saw earlier, won the very popular ImageNet competition, looking to try to figure out  from one and a half million images what they're pictures of. As of 2014, we're now down to a six percent error rate in image recognition. This is better than people, again.So machines really are doing an extraordinarily good job of this, and it is now being used in industry. For example, Google announced last year that they had mapped every single location in France in two hours, and the way they did it was that they fed street view images into a deep learning algorithm to recognize and read street numbers. Imagine how long it would have taken before: dozens of people, many years. This is also happening in China. Baidu is kind of  the Chinese Google, I guess, and what you see here in the top left is an example of a picture that I uploaded to Baidu's deep learning system, and underneath you can see that the system has understood what that picture is and found similar images. The similar images actually have similar backgrounds, similar directions of the faces, even some with their tongue out. This is not clearly looking at the text of a web page. All I uploaded was an image. So we now have computers which really understand what they see and can therefore search databases of hundreds of millions of images in real time.So what does it mean now that computers can see? Well, it's not just  that computers can see. In fact, deep learning has done more than that. Complex, nuanced sentences like this one are now understandable with deep learning algorithms. As you can see here, this Stanford-based system showing the red dot at the top has figured out that this sentence is expressing negative sentiment. Deep learning now in fact is near human performance at understanding what sentences are about and what it is saying about those things. Also, deep learning has been used to read Chinese, again at about native Chinese speaker level. This algorithm developed out of Switzerland by people, none of whom speak or understand any Chinese. As I say, using deep learning is about the best system in the world for this, even compared to native human understanding.This is a system that we put together at my company which shows putting all this stuff together. These are pictures which have no text attached, and as I'm typing in here sentences, in real time it's understanding these pictures and figuring out what they're about and finding pictures that are similar to the text that I'm writing. So you can see, it's actually understanding my sentences and actually understanding these pictures. I know that you've seen something like this on Google, where you can type in things and it will show you pictures, but actually what it's doing is it's searching the webpage for the text. This is very different from actually understanding the images. This is something that computers have only been able to do for the first time in the last few months.So we can see now that computers can not only see but they can also read, and, of course, we've shown that they can understand what they hear. Perhaps not surprising now that I'm going to tell you they can write. Here is some text that I generated using a deep learning algorithm yesterday. And here is some text that an algorithm out of Stanford generated. Each of these sentences was generated by a deep learning algorithm to describe each of those pictures. This algorithm before has never seen a man in a black shirt playing a guitar. It's seen a man before, it's seen black before, it's seen a guitar before, but it has independently generated this novel description of this picture. We're still not quite at human performance here, but we're close. In tests, humans prefer the computer-generated caption one out of four times. Now this system is now only two weeks old, so probably within the next year, the computer algorithm will be well past human performance at the rate things are going. So computers can also write.So we put all this together and it leads to very exciting opportunities. For example, in medicine, a team in Boston announced that they had discovered dozens of new clinically relevant features of tumors which help doctors make a prognosis of a cancer. Very similarly, in Stanford, a group there announced that, looking at tissues under magnification, they've developed  a machine learning-based system which in fact is better than human pathologists at predicting survival rates for cancer sufferers. In both of these cases, not only were the predictions more accurate, but they generated new insightful science. In the radiology case, they were new clinical indicators that humans can understand. In this pathology case, the computer system actually discovered that the cells around the cancer are as important as the cancer cells themselves in making a diagnosis. This is the opposite of what pathologists had been taught for decades. In each of those two cases, they were systems developed by a combination of medical experts and machine learning experts, but as of last year, we're now beyond that too. This is an example of identifying cancerous areas of human tissue under a microscope. The system being shown here can identify those areas more accurately, or about as accurately, as human pathologists, but was built entirely with deep learning using no medical expertise by people who have no background in the field. Similarly, here, this neuron segmentation. We can now segment neurons about as accurately as humans can, but this system was developed with deep learning using people with no previous  background in medicine.So myself, as somebody with no previous background in medicine, I seem to be entirely well qualified to start a new medical company, which I did. I was kind of terrified of doing it, but the theory seemed to suggest that it ought to be possible to do very useful medicine using just these data analytic techniques. And thankfully, the feedback has been fantastic, not just from the media but from the medical community, who have been very supportive. The theory is that we can take the middle part of the medical process and turn that into data analysis as much as possible, leaving doctors to do what they're best at. I want to give you an example. It now takes us about 15 minutes to generate a new medical diagnostic test and I'll show you that in real time now, but I've compressed it down to  three minutes by cutting some pieces out. Rather than showing you creating a medical diagnostic test, I'm going to show you  a diagnostic test of car images, because that's something we can all understand.So here we're starting with  about 1.5 million car images, and I want to create something that can split them into the angle of the photo that's being taken. So these images are entirely unlabeled, so I have to start from scratch. With our deep learning algorithm, it can automatically identify areas of structure in these images. So the nice thing is that the human and the computer can now work together. So the human, as you can see here, is telling the computer about areas of interest which it wants the computer then to try and use to improve its algorithm. Now, these deep learning systems actually are in 16,000-dimensional space, so you can see here the computer rotating this through that space, trying to find new areas of structure. And when it does so successfully, the human who is driving it can then point out the areas that are interesting. So here, the computer has successfully found areas, for example, angles. So as we go through this process, we're gradually telling the computer more and more about the kinds of structures we're looking for. You can imagine in a diagnostic test this would be a pathologist identifying areas of pathosis, for example, or a radiologist indicating potentially troublesome nodules. And sometimes it can be difficult for the algorithm. In this case, it got kind of confused. The fronts and the backs of the cars are all mixed up. So here we have to be a bit more careful, manually selecting these fronts as opposed to the backs, then telling the computer that this is a type of group that we're interested in.So we do that for a while, we skip over a little bit, and then we train the machine learning algorithm based on these couple of hundred things, and we hope that it's gotten a lot better. You can see, it's now started to fade some of these pictures out, showing us that it already is recognizing how to understand some of these itself. We can then use this concept of similar images, and using similar images, you can now see, the computer at this point is able to entirely find just the fronts of cars. So at this point, the human can tell the computer, okay, yes, you've done a good job of that.Sometimes, of course, even at this point it's still difficult to separate out groups. In this case, even after we let the computer try to rotate this for a while, we still find that the left sides and the right sides pictures are all mixed up together. So we can again give the computer some hints, and we say, okay, try and find a projection that separates out the left sides and the right sides as much as possible using this deep learning algorithm. And giving it that hint \\u2014 ah, okay, it's been successful. It's managed to find a way of thinking about these objects that's separated out these together.So you get the idea here. This is a case not where the human is being replaced by a computer, but where they're working together. What we're doing here is we're replacing something that used to take a team of five or six people about seven years and replacing it with something that takes 15 minutes for one person acting alone.So this process takes about four or five iterations. You can see we now have 62 percent of our 1.5 million images  classified correctly. And at this point, we can start to quite quickly grab whole big sections, check through them to make sure that there's no mistakes. Where there are mistakes, we can let the computer know about them. And using this kind of process for each of the different groups, we are now up to an 80 percent success rate in classifying the 1.5 million images. And at this point, it's just a case of finding the small number that aren't classified correctly, and trying to understand why. And using that approach, by 15 minutes we get to 97 percent classification rates.So this kind of technique could allow us to fix a major problem, which is that there's a lack of medical expertise in the world. The World Economic Forum says that there's between a 10x and a 20x shortage of physicians in the developing world, and it would take about 300 years to train enough people to fix that problem. So imagine if we can help enhance their efficiency using these deep learning approaches?So I'm very excited about the opportunities. I'm also concerned about the problems. The problem here is that every area in blue on this map is somewhere where services are over 80 percent of employment. What are services? These are services. These are also the exact things that computers have just learned how to do. So 80 percent of the world's employment in the developed world is stuff that computers  have just learned how to do. What does that mean? Well, it'll be fine. They'll be replaced by other jobs. For example, there will be more jobs for data scientists. Well, not really. It doesn't take data scientists  very long to build these things. For example, these four algorithms were all built by the same guy. So if you think, oh,  it's all happened before, we've seen the results in the past of when new things come along and they get replaced by new jobs, what are these new jobs going to be? It's very hard for us to estimate this, because human performance grows at this gradual rate, but we now have a system, deep learning, that we know actually grows in capability exponentially. And we're here. So currently, we see the things around us and we say, \\\"Oh, computers are still pretty dumb.\\\" Right? But in five years' time, computers will be off this chart. So we need to be starting to think about this capability right now.We have seen this once before, of course. In the Industrial Revolution, we saw a step change in capability thanks to engines. The thing is, though, that after a while, things flattened out. There was social disruption, but once engines were used  to generate power in all the situations, things really settled down. The Machine Learning Revolution is going to be very different from the Industrial Revolution, because the Machine Learning Revolution, it never settles down. The better computers get at intellectual activities, the more they can build better computers to be better at intellectual capabilities, so this is going to be a kind of change that the world has actually never experienced before, so your previous understanding of what's possible is different.This is already impacting us. In the last 25 years, as capital productivity has increased, labor productivity has been flat, in fact even a little bit down.So I want us to start having this discussion now. I know that when I often tell people about this situation, people can be quite dismissive. Well, computers can't really think, they don't emote, they don't understand poetry, we don't really understand how they work. So what? Computers right now can do the things that humans spend most of their time being paid to do, so now's the time to start thinking about how we're going to adjust our social structures and economic structures to be aware of this new reality. Thank you. (Applause)\",\n          \"It's a great honor today to share with you The Digital Universe, which was created for humanity to really see where we are in the universe. And so I think we can roll the video that we have.[The Himalayas.](Music)The flat horizon that we've evolved with has been a metaphor for the infinite: unbounded resources and unlimited capacity for disposal of waste. It wasn't until we really left Earth, got above the atmosphere and had seen the horizon bend back on itself, that we could understand our planet as a limited condition. The Digital Universe Atlas has been built at the American Museum of Natural History over the past 12 years. We maintain that, put that together as a project to really chart the universe across all scales. What we see here are satellites around the Earth and the Earth in proper registration against the universe, as we see. NASA supported this work 12 years ago as part of the rebuilding of the Hayden Planetarium so that we would share this with the world.The Digital Universe is the basis of our space show productions that we do \\u2014 our main space shows in the dome. But what you see here is the result of, actually, internships that we hosted with Linkoping University in Sweden. I've had 12 students work on this for their graduate work, and the result has been this software called Uniview and a company called SCISS in Sweden. This software allows interactive use, so this actual flight path and movie that we see here was actually flown live. I captured this live from my laptop in a cafe called Earth Matters on the Lower East Side of Manhattan, where I live, and it was done as a collaborative project with the Rubin Museum of Himalayan Art for an exhibit on comparative cosmology.And so as we move out, we see continuously from our planet all the way out into the realm of galaxies, as we see here, light-travel time, giving you a sense of how far away we are. As we move out, the light from these distant galaxies have taken so long, we're essentially backing up into the past. We back so far up we're finally seeing a containment around us \\u2014 the afterglow of the Big Bang. This is the WMAP microwave background that we see. We'll fly outside it here, just to see this sort of containment. If we were outside this, it would almost be meaningless, in the sense as before time. But this our containment of the visible universe. We know the universe is bigger than that which we can see.Coming back quickly, we see here the radio sphere that we jumped out of in the beginning, but these are positions, the latest positions of exoplanets that we've mapped, and our sun here, obviously, with our own solar system. What you're going to see \\u2014 we're going to have to jump in here pretty quickly between several orders of magnitude to get down to where we see the solar system \\u2014 these are the paths of Voyager 1, Voyager 2, Pioneer 11 and Pioneer 10, the first four spacecraft to have left the solar system. Coming in closer, picking up Earth, orbit of the Moon, and we see the Earth. This map can be updated, and we can add in new data.I know Dr. Carolyn Porco is the camera P.I. for the Cassini mission. But here we see the complex trajectory of the Cassini mission color coded for different mission phases, ingeniously developed so that 45 encounters with the largest moon, Titan, which is larger that the planet Mercury, diverts the orbit into different parts of mission phase.This software allows us to come close and look at parts of this. This software can also be networked between domes. We have a growing user base of this, and we network domes. And we can network between domes and classrooms. We're actually sharing tours of the universe with the first sub-Saharan planetarium in Ghana as well as new libraries that have been built in the ghettos in Columbia and a high school in Cambodia. And the Cambodians have actually controlled the Hayden Planetarium from their high school.This is an image from Saturday, photographed by the Aqua satellite, but through the Uniview software. So you're seeing the edge of the Earth. This is Nepal. This is, in fact, right here is the valley of Lhasa, right here in Tibet. But we can see the haze from fires and so forth in the Ganges valley down below in India. This is Nepal and Tibet.And just in closing, I'd just like to say this beautiful world that we live on \\u2014 here we see a bit of the snow that some of you may have had to brave in coming out \\u2014 so I'd like to just say that what the world needs now is a sense of being able to look at ourselves in this much larger condition now and a much larger sense of what home is. Because our home is the universe, and we are the universe, essentially. We carry that in us. And to be able to see our context in this larger sense at all scales helps us all, I think, in understanding where we are and who we are in the universe.Thank you.(Applause)\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"https://www.ted.com/talks/aaron_o_connell_making_sense_of_a_visible_quantum_object\\n\",\n          \"https://www.ted.com/talks/jeremy_howard_the_wonderful_and_terrifying_implications_of_computers_that_can_learn\\n\",\n          \"https://www.ted.com/talks/carter_emmart_demos_a_3d_atlas_of_the_universe\\n\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recommendations for talk at URL: https://www.ted.com/talks/al_seckel_says_our_brains_are_mis_wired\n",
            "\n",
            "65     https://www.ted.com/talks/kary_mullis_on_what_...\n",
            "456    https://www.ted.com/talks/gregory_stock_to_upg...\n",
            "97     https://www.ted.com/talks/dan_ariely_asks_are_...\n",
            "223    https://www.ted.com/talks/ricardo_semler_how_t...\n",
            "104    https://www.ted.com/talks/rob_legato_the_art_o...\n",
            "323    https://www.ted.com/talks/nic_marks_the_happy_...\n",
            "153    https://www.ted.com/talks/scott_dinsmore_how_t...\n",
            "57     https://www.ted.com/talks/alison_jackson_looks...\n",
            "432    https://www.ted.com/talks/paola_antonelli_prev...\n",
            "120    https://www.ted.com/talks/vik_muniz_makes_art_...\n",
            "Name: url, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beyond n-grams: word embeddings"
      ],
      "metadata": {
        "id": "AxGt3ek5T8N-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sent = \"I like apples and oranges\"\n",
        "\n",
        "# Create the doc object\n",
        "doc = nlp(sent)\n",
        "\n",
        "# Compute pairwise similarity scores\n",
        "for token1 in doc:\n",
        "  for token2 in doc:\n",
        "    print(token1.text, token2.text, token1.similarity(token2))\n",
        "\n",
        "\n",
        "# Good job! Notice how the words 'apples' and 'oranges' have the highest pairwaise similarity score.\n",
        "# This is expected as they are both fruits and are more related to each other than any other pair of words."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6weHjGsVKl0",
        "outputId": "88cbf77a-b2f8-4a01-b1fe-78ab07c84e65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I I 1.0\n",
            "I like 0.19823068380355835\n",
            "I apples 0.16132277250289917\n",
            "I and -0.04532375931739807\n",
            "I oranges 0.06013873219490051\n",
            "like I 0.19823068380355835\n",
            "like like 1.0\n",
            "like apples 0.02722913958132267\n",
            "like and -0.019127340987324715\n",
            "like oranges 0.017664654180407524\n",
            "apples I 0.16132277250289917\n",
            "apples like 0.02722913958132267\n",
            "apples apples 1.0\n",
            "apples and -0.00598952965810895\n",
            "apples oranges 0.39695537090301514\n",
            "and I -0.04532375931739807\n",
            "and like -0.019127340987324715\n",
            "and apples -0.00598952965810895\n",
            "and and 1.0\n",
            "and oranges -0.005365171004086733\n",
            "oranges I 0.06013873219490051\n",
            "oranges like 0.017664654180407524\n",
            "oranges apples 0.39695537090301514\n",
            "oranges and -0.005365171004086733\n",
            "oranges oranges 1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-40-825226101.py:9: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Token.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(token1.text, token2.text, token1.similarity(token2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Computing similarity of Pink Floyd songs**  \n",
        "In this final exercise, you have been given lyrics of three songs by the British band Pink Floyd, namely 'High Hopes', 'Hey You' and 'Mother'. The lyrics to these songs are available as hopes, hey and mother respectively.\n",
        "\n",
        "Your task is to compute the pairwise similarity between mother and hopes, and mother and hey.\n",
        "\n"
      ],
      "metadata": {
        "id": "HGPuPWRtV2hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mother = \"\\nMother do you think they'll drop the bomb?\\nMother do you think they'll like this song?\\nMother do you think they'll try to break my balls?\\nOoh, ah\\nMother should I build the wall?\\nMother should I run for President?\\nMother should I trust the government?\\nMother will they put me in the firing mine?\\nOoh ah,\\nIs it just a waste of time?\\nHush now baby, baby, don't you cry.\\nMama's gonna make all your nightmares come true.\\nMama's gonna put all her fears into you.\\nMama's gonna keep you right here under her wing.\\nShe won't let you fly, but she might let you sing.\\nMama's gonna keep baby cozy and warm.\\nOoh baby, ooh baby, ooh baby,\\nOf course mama's gonna help build the wall.\\nMother do you think she's good enough, for me?\\nMother do you think she's dangerous, to me?\\nMother will she tear your little boy apart?\\nOoh ah,\\nMother will she break my heart?\\nHush now baby, baby don't you cry.\\nMama's gonna check out all your girlfriends for you.\\nMama won't let anyone dirty get through.\\nMama's gonna wait up until you get in.\\nMama will always find out where you've been.\\nMama's gonna keep baby healthy and clean.\\nOoh baby, ooh baby, ooh baby,\\nYou'll always be baby to me.\\nMother, did it need to be so high?\\n\"\n",
        "hopes = \"\\nBeyond the horizon of the place we lived when we were young\\nIn a world of magnets and miracles\\nOur thoughts strayed constantly and without boundary\\nThe ringing of the division bell had begun\\nAlong the Long Road and on down the Causeway\\nDo they still meet there by the Cut\\nThere was a ragged band that followed in our footsteps\\nRunning before times took our dreams away\\nLeaving the myriad small creatures trying to tie us to the ground\\nTo a life consumed by slow decay\\nThe grass was greener\\nThe light was brighter\\nWhen friends surrounded\\nThe nights of wonder\\nLooking beyond the embers of bridges glowing behind us\\nTo a glimpse of how green it was on the other side\\nSteps taken forwards but sleepwalking back again\\nDragged by the force of some in a tide\\nAt a higher altitude with flag unfurled\\nWe reached the dizzy heights of that dreamed of world\\nEncumbered forever by desire and ambition\\nThere's a hunger still unsatisfied\\nOur weary eyes still stray to the horizon\\nThough down this road we've been so many times\\nThe grass was greener\\nThe light was brighter\\nThe taste was sweeter\\nThe nights of wonder\\nWith friends surrounded\\nThe dawn mist glowing\\nThe water flowing\\nThe endless river\\nForever and ever\\n\"\n",
        "hey = \"\\nHey you, out there in the cold\\nGetting lonely, getting old\\nCan you feel me?\\nHey you, standing in the aisles\\nWith itchy feet and fading smiles\\nCan you feel me?\\nHey you, don't help them to bury the light\\nDon't give in without a fight\\nHey you out there on your own\\nSitting naked by the phone\\nWould you touch me?\\nHey you with you ear against the wall\\nWaiting for someone to call out\\nWould you touch me?\\nHey you, would you help me to carry the stone?\\nOpen your heart, I'm coming home\\nBut it was only fantasy\\nThe wall was too high\\nAs you can see\\nNo matter how he tried\\nHe could not break free\\nAnd the worms ate into his brain\\nHey you, out there on the road\\nAlways doing what you're told\\nCan you help me?\\nHey you, out there beyond the wall\\nBreaking bottles in the hall\\nCan you help me?\\nHey you, don't tell me there's no hope at all\\nTogether we stand, divided we fall\\n\"\n",
        "\n",
        "\n",
        "# Create Doc objects\n",
        "mother_doc = nlp(mother)\n",
        "hopes_doc = nlp(hopes)\n",
        "hey_doc = nlp(hey)\n",
        "\n",
        "# Print similarity between mother and hopes\n",
        "print(mother_doc.similarity(hopes_doc))\n",
        "# Print similarity between mother and hey\n",
        "print(mother_doc.similarity(hey_doc))\n",
        "\n",
        "# Notice that 'Mother' and 'Hey You' have a similarity score of 0.9 whereas 'Mother' and 'High Hopes' has a score of only 0.6.\n",
        "# This is probably because 'Mother' and 'Hey You' were both songs from the same album 'The Wall' and were penned by Roger Waters.\n",
        "# On the other hand, 'High Hopes' was a part of the album 'Division Bell' with lyrics by David Gilmour and his wife, Penny Samson.\n",
        "# Treat yourself by listening to these songs. They're some of the best!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Ktt792JWA-N",
        "outputId": "39bfc66e-9b45-4a91-c89a-d1d90433aebb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.36541473865509033\n",
            "0.7763113975524902\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-42-2896140850.py:12: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(mother_doc.similarity(hopes_doc))\n",
            "/tmp/ipython-input-42-2896140850.py:14: UserWarning: [W007] The model you're using has no word vectors loaded, so the result of the Doc.similarity method will be based on the tagger, parser and NER, which may not give useful similarity judgements. This may happen if you're using one of the small models, e.g. `en_core_web_sm`, which don't ship with word vectors and only use context-sensitive tensors. You can always add your own word vectors, or use one of the larger models instead if available.\n",
            "  print(mother_doc.similarity(hey_doc))\n"
          ]
        }
      ]
    }
  ]
}