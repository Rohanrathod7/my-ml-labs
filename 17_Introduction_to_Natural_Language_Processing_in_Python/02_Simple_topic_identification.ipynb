{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoKB+DkDqeLAB+EIFPNwv5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rohanrathod7/my-ml-labs/blob/main/17_Introduction_to_Natural_Language_Processing_in_Python/02_Simple_topic_identification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. imple topic identification\n"
      ],
      "metadata": {
        "id": "69GNV3tpDLy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUPpAj0HoP6i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime as dt\n",
        "# Import confusion matrix and train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
        "from sklearn.linear_model import Ridge, Lasso, LogisticRegression, LinearRegression\n",
        "from scipy.cluster.hierarchy import linkage, dendrogram\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/15_Hyperparameter_Tuning_in_Python/Dataset/results_df.csv\"\n",
        "# Read the CSV file\n",
        "# Apply pd.to_numeric only to relevant columns, excluding 'text'\n",
        "results_df = pd.read_csv(url)\n",
        "\n",
        "\n",
        "display(results_df.head())\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "u8UlyiPU6rzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word counts with bag-of-words"
      ],
      "metadata": {
        "id": "JS39WRZSSSWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Bag-of-words picker**  \n",
        "It's time for a quick check on your understanding of bag-of-words. Which of the below options, with basic nltk tokenization, map the bag-of-words for the following text?\n",
        "\n",
        "\"The cat is in the box. The cat box.\"\n",
        "\n",
        "    -> ('The', 2), ('box', 2), ('.', 2), ('cat', 2), ('is', 1), ('in', 1), ('the', 1)"
      ],
      "metadata": {
        "id": "e6qqQO7XS3aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building a Counter with bag-of-words**  \n",
        "In this exercise, you'll build your first (in this course) bag-of-words counter using a Wikipedia article, which has been pre-loaded as article. Try doing the bag-of-words without looking at the full `article` text, and guessing what the topic is! If you'd like to peek at the title at the end, we've included it as `article_title`. Note that this article text has had very little preprocessing from the raw Wikipedia database entry.\n",
        "\n",
        "word_tokenize has been imported for you."
      ],
      "metadata": {
        "id": "PkiVv8EBTTJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "zZZmA0bWU1-y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "# Download the required NLTK data\n",
        "\n",
        "\n",
        "\n",
        "url = \"https://raw.githubusercontent.com/Rohanrathod7/my-ml-labs/main/17_Introduction_to_Natural_Language_Processing_in_Python/dataset/News articles/articles.txt\"\n",
        "\n",
        "try:\n",
        "    # Fetch the content from the URL\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()  # Raise an exception for bad status codes\n",
        "    article = response.text\n",
        "\n",
        "\n",
        "\n",
        "    # Display the first 500 characters\n",
        "    print(article[:500])\n",
        "\n",
        "    # Tokenize the article: tokens\n",
        "    tokens = word_tokenize(article)\n",
        "\n",
        "    # Convert the tokens into lowercase: lower_tokens\n",
        "    lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "    # Create a Counter with the lowercase tokens: bow_simple\n",
        "    bow_simple = Counter(lower_tokens)\n",
        "\n",
        "    # Print the 10 most common tokens\n",
        "    print(bow_simple.most_common(10))\n",
        "\n",
        "\n",
        "except requests.exceptions.RequestException as e:\n",
        "    print(f\"Error fetching the file: {e}\")\n",
        "    print(\"Please check if the URL is correct and accessible.\")"
      ],
      "metadata": {
        "id": "2aiKxRfkTRMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocessing steps**  \n",
        "Which of the following are useful text preprocessing steps?\n",
        "\n",
        "\n",
        "    Lemmatization, lowercasing, removing unwanted tokens."
      ],
      "metadata": {
        "id": "UTOI_hYWWIlv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Text preprocessing practice**  \n",
        "Now, it's your turn to apply the techniques you've learned to help clean up text for better NLP results. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n",
        "\n",
        "You start with the same tokens you created in the last exercise: lower_tokens. You also have the Counter class imported."
      ],
      "metadata": {
        "id": "XpmP7-IEWRDo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "D5SkNtoiXtD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "english_stops = ['i',\n",
        " 'me','my','myself','we','our','ours','ourselves','you','your','yours','yourself','yourselves','he','him','his','himself','she','her','hers','herself','it','its','itself','they','them','their',\n",
        " 'theirs','themselves','what','which','who','whom','this','that','these','those','am','is','are','was','were','be','been','being','have','has','had','having','do','does','did','doing',\n",
        " 'a','an','the','and','but','if','or','because','as','until','while','of','at','by','for','with','about','against','between','into','through','during','before','after','above',\n",
        " 'below','to','from','up','down','in','out','on','off','over','under','again','further','then','once','here','there','when','where','why','how','all','any','both','each','few','more','most',\n",
        " 'other','some','such','no','nor','not','only','own','same','so','than','too','very','s','t','can','will','just','don','should','now','d','ll','m','o','re','ve','y','ain','aren','couldn','didn',\n",
        " 'doesn','hadn','hasn','haven','isn','ma','mightn','mustn','needn','shan','shouldn','wasn','weren','won','wouldn','']\n",
        "\n",
        "# Retain alphabetic words: alpha_only\n",
        "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
        "\n",
        "# Remove all stop words: no_stops\n",
        "no_stops = [t for t in alpha_only if t not in english_stops]\n",
        "\n",
        "# Instantiate the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Lemmatize all tokens into a new list: lemmatized\n",
        "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
        "\n",
        "# Create the bag-of-words: bow\n",
        "bow = Counter(lemmatized)\n",
        "\n",
        "# Print the 10 most common tokens\n",
        "print(bow.most_common(10))\n"
      ],
      "metadata": {
        "id": "xpt8YG7yXVuP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What are word vectors?**  \n",
        "What are word vectors and how do they help with NLP?\n",
        "\n",
        "    Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. They give us insight into relationships between words in a corpus."
      ],
      "metadata": {
        "id": "22hcmBIjZ9d4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating and querying a corpus with gensim**  \n",
        "It's time to apply the methods you learned in the previous video to create your first gensim dictionary and corpus!\n",
        "\n",
        "You'll use these data structures to investigate word trends and potential interesting topics in your document set. To get started, we have imported a few additional messy articles from Wikipedia, which were preprocessed by lowercasing all words, tokenizing them, and removing stop words and punctuation. These were then stored in a list of document tokens called articles. You'll need to do some light preprocessing and then generate the gensim dictionary and corpus."
      ],
      "metadata": {
        "id": "a4ZqntyqeQrM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f661e4ac",
        "outputId": "17b72fee-803c-48d3-c23e-8107a8dc665a"
      },
      "source": [
        "%pip install gensim"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.11/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.3.0.post1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Dictionary\n",
        "from gensim.corpora.dictionary import Dictionary\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Assuming 'articles' is a list of strings, where each string is an article\n",
        "# If 'articles' is not defined, you'll need to define it by loading your data.\n",
        "\n",
        "# Download the required NLTK data for tokenization if not already downloaded\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize each article\n",
        "tokenized_articles = [word_tokenize(article) for article in article]\n",
        "\n",
        "# Create a Dictionary from the tokenized articles: dictionary\n",
        "dictionary = Dictionary(tokenized_articles)\n",
        "\n",
        "# Select the id for \"computer\": computer_id\n",
        "computer_id = dictionary.token2id.get(\"computer\")\n",
        "\n",
        "# Use computer_id with the dictionary to print the word\n",
        "if computer_id is not None:\n",
        "    print(dictionary.get(computer_id))\n",
        "else:\n",
        "    print(\"'computer' not found in the dictionary.\")\n",
        "\n",
        "\n",
        "# Create a MmCorpus: corpus\n",
        "corpus = [dictionary.doc2bow(article) for article in tokenized_articles]\n",
        "\n",
        "# Print the first 10 word ids with their frequency counts from the fifth document\n",
        "if len(corpus) > 4:\n",
        "    print(corpus[4][:10])\n",
        "else:\n",
        "    print(\"Not enough documents in the corpus to display the fifth document.\")"
      ],
      "metadata": {
        "id": "7gWuaHp4eUDC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gensim bag-of-words\n",
        "\n",
        "Now, you'll use your new gensim corpus and dictionary to see the most common terms per document and across all documents. You can use your dictionary to look up the terms. Take a guess at what the topics are and feel free to explore more documents in the IPython Shell!\n",
        "\n",
        "You have access to the dictionary and corpus objects you created in the previous exercise, as well as the Python defaultdict and itertools to help with the creation of intermediate data structures for analysis.\n",
        "\n",
        "- defaultdict allows us to initialize a dictionary that will assign a default value to non-existent keys. By supplying the argument int, we are able to ensure that any non-existent keys are automatically assigned a default value of 0. This makes it ideal for storing the counts of words in this exercise.\n",
        "\n",
        "- `itertools.chain.from_iterable()` allows us to iterate through a set of sequences as if they were one continuous sequence. Using this function, we can easily iterate through our corpus object (which is a list of lists).\n",
        "\n",
        "The fifth document from corpus is stored in the variable doc, which has been sorted in descending order.\n",
        "\n"
      ],
      "metadata": {
        "id": "13tCJMn1eyd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "import itertools\n",
        "\n",
        "# Save the fifth document: doc\n",
        "doc = corpus[4]\n",
        "\n",
        "# Sort the doc for frequency: bow_doc\n",
        "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words of the document alongside the count\n",
        "for word_id, word_count in bow_doc[:5]:\n",
        "    print(dictionary.get(word_id), word_count)\n",
        "\n",
        "# Create the defaultdict: total_word_count\n",
        "total_word_count = defaultdict(int)\n",
        "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
        "    total_word_count[word_id] += word_count\n",
        "\n",
        "# Create a sorted list from the defaultdict: sorted_word_count\n",
        "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 words across all documents alongside the count\n",
        "for word_id, word_count in sorted_word_count[:5]:\n",
        "    print(dictionary.get(word_id), word_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VY1IG--_h2g7",
        "outputId": "5d293423-481c-4f82-9e1a-e4386a79b92f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "g 1\n",
            "e 2662\n",
            "t 2080\n",
            "a 1900\n",
            "o 1763\n",
            "i 1729\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is tf-idf?**\n",
        "You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by multiplying term frequency with inverse document frequency.\n",
        "\n",
        "Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
        "\n",
        "Which of the below options is correct?\n",
        "\n",
        "\n",
        "    (5 / 100) * log(200 / 20)"
      ],
      "metadata": {
        "id": "OJChmkADo-uL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tf-idf with Wikipedia**  \n",
        "Now it's your turn to determine new significant terms for your corpus by applying gensim's tf-idf. You will again have access to the same corpus and dictionary objects you created in the previous exercises - dictionary, corpus, and doc. Will tf-idf make for more interesting results on the document level?\n",
        "\n",
        "TfidfModel has been imported for you from gensim.models.tfidfmodel."
      ],
      "metadata": {
        "id": "q3IAmu3spJJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TfidfModel\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "# Create a new TfidfModel using the corpus: tfidf\n",
        "tfidf = TfidfModel(corpus)\n",
        "\n",
        "# Calculate the tfidf weights of doc: tfidf_weights\n",
        "tfidf_weights = tfidf[doc]\n",
        "\n",
        "# Print the first five weights\n",
        "print(tfidf_weights[:5])\n",
        "\n",
        "# Sort the weights from highest to lowest: sorted_tfidf_weights\n",
        "sorted_tfidf_weights = sorted(tfidf_weights, key=lambda w: w[1], reverse=True)\n",
        "\n",
        "# Print the top 5 weighted words\n",
        "for term_id, weight in sorted_tfidf_weights[:5]:\n",
        "    print(dictionary.get(term_id), weight)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nti42c8Kpzh-",
        "outputId": "c01f4999-cf02-45b3-d796-af6e76df8244"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(4, 1.0)]\n",
            "g 1.0\n"
          ]
        }
      ]
    }
  ]
}